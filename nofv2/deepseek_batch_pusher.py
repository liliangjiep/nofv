import json
import asyncio
import logging
import aiohttp
from aiohttp_socks import ProxyConnector
import html
from decimal import Decimal
import time
import re
from concurrent.futures import ThreadPoolExecutor
from config import CLAUDE_API_KEY, CLAUDE_MODEL, CLAUDE_URL, AI_PROVIDER, timeframes, PROXY
from config import MIMO_API_KEY, MIMO_MODEL, MIMO_URL
from database import redis_client
from volume_stats import get_open_interest, get_funding_rate, get_24hr_change
from account_positions import account_snapshot, tp_sl_cache
from trend_alignment import calculate_trend_alignment

_preload_executor = ThreadPoolExecutor(max_workers=12)

KEY_REQ = "deepseek_analysis_request_history"
KEY_RES = "deepseek_analysis_response_history"

batch_cache = {}

# ================== å…¨å±€ HTTP Sessionï¼ˆè¿›ç¨‹çº§ï¼‰ ==================
_http_session = None
_http_session_no_proxy = None  # ç”¨äºæœ¬åœ°è¯·æ±‚ï¼ˆä¸èµ°ä»£ç†ï¼‰

async def init_http_session():
    """
    åˆå§‹åŒ–å…¨å±€ HTTP Sessionï¼ˆåªåšä¸€æ¬¡ï¼‰
    æ”¯æŒä»£ç†é…ç½®
    """
    global _http_session, _http_session_no_proxy

    if _http_session is None or _http_session.closed:
        if PROXY:
            connector = ProxyConnector.from_url(PROXY)
            _http_session = aiohttp.ClientSession(connector=connector)
            print(f"ğŸŒ å…¨å±€ HTTP Session å·²åˆå§‹åŒ– (ä»£ç†: {PROXY})")
        else:
            _http_session = aiohttp.ClientSession()
            print("ğŸŒ å…¨å±€ HTTP Session å·²åˆå§‹åŒ–")
    
    # åˆå§‹åŒ–æ— ä»£ç† Sessionï¼ˆç”¨äºæœ¬åœ°è¯·æ±‚ï¼‰
    if _http_session_no_proxy is None or _http_session_no_proxy.closed:
        _http_session_no_proxy = aiohttp.ClientSession()
        print("ğŸŒ æ— ä»£ç† HTTP Session å·²åˆå§‹åŒ–ï¼ˆç”¨äºæœ¬åœ°è¯·æ±‚ï¼‰")

async def get_http_session(url: str = None) -> aiohttp.ClientSession:
    """
    è·å– HTTP Session
    å¦‚æœ URL æ˜¯æœ¬åœ°åœ°å€ï¼Œè¿”å›æ— ä»£ç†çš„ Session
    """
    if _http_session is None or _http_session.closed:
        raise RuntimeError("HTTP Session å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_http_session()")
    
    # æœ¬åœ°åœ°å€ä¸èµ°ä»£ç†
    if url and ("localhost" in url or "127.0.0.1" in url):
        if _http_session_no_proxy is None or _http_session_no_proxy.closed:
            raise RuntimeError("æ— ä»£ç† HTTP Session å°šæœªåˆå§‹åŒ–")
        return _http_session_no_proxy
    
    return _http_session

async def close_http_session():
    """
    ç¨‹åºé€€å‡ºæ—¶è°ƒç”¨ï¼Œä¼˜é›…å…³é—­
    """
    global _http_session, _http_session_no_proxy

    if _http_session is not None:
        await _http_session.close()
        _http_session = None
        print("ğŸ›‘ å…¨å±€ HTTP Session å·²å…³é—­")
    
    if _http_session_no_proxy is not None:
        await _http_session_no_proxy.close()
        _http_session_no_proxy = None
        print("ğŸ›‘ æ— ä»£ç† HTTP Session å·²å…³é—­")

def json_safe_dumps(obj):
    import numpy as np
    def default_handler(x):
        if isinstance(x, Decimal):
            return float(x)
        if isinstance(x, (np.bool_, np.integer)):
            return int(x)
        if isinstance(x, np.floating):
            return float(x)
        if isinstance(x, np.ndarray):
            return x.tolist()
        return str(x)
    
    return json.dumps(
        obj,
        ensure_ascii=False,
        default=default_handler
    )

# ================== Batch ç®¡ç† ==================
def add_to_batch(symbol, interval, indicators=None):
    if symbol not in batch_cache:
        batch_cache[symbol] = {}

    payload = {}
    if indicators is not None:
        payload["indicators"] = indicators

    batch_cache[symbol][interval] = payload

def _is_ready_for_push():
    """
    æ£€æŸ¥ batch_cache æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªå¸ç§æœ‰æ•°æ®ã€‚
    æ”¾å®½è¦æ±‚ï¼Œä¸å†å¼ºåˆ¶æ¯ä¸ªå‘¨æœŸéƒ½å¿…é¡»å®Œæ•´ã€‚
    """
    if not batch_cache:
        print("âš ï¸ batch_cache ä¸ºç©ºï¼Œæ— æ³•æŠ•å–‚")
        return False

    ready_symbols = []
    for symbol, cycles in batch_cache.items():
        if cycles:  # è‡³å°‘æœ‰ä¸€ä¸ªå‘¨æœŸæ•°æ®
            ready_symbols.append(symbol)
        else:
            print(f"âš ï¸ {symbol} ç¼ºå°‘ä»»ä½•å‘¨æœŸæ•°æ®")

    if not ready_symbols:
        print("âš ï¸ æ²¡æœ‰å¸ç§æ»¡è¶³æŠ•å–‚æ¡ä»¶")
        return False

    print(f"âœ… å‡†å¤‡æŠ•å–‚çš„å¸ç§: {ready_symbols}")
    return True

def sentiment_to_signal(score):
    if score >= 85:
        return "ğŸš¨ æç«¯è¿‡çƒ­ | è­¦æƒ•é¡¶éƒ¨åè½¬"
    if score >= 70:
        return "ğŸŸ¢ ç‰›åŠ¿å¼ºåŠ² |"
    if score >= 50:
        return "âšª ä¸­æ€§éœ‡è¡ | è€å¿ƒç­‰å¾…çªç ´"
    if score >= 30:
        return "ğŸŸ¡ ææ…Œç¼“è§£"
    return "ğŸ”¥ æåº¦ææ…Œ"

def _read_prompt():
    try:
        with open("prompt.txt", "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return "You are a crypto short-term trend trader."

# ================== API é¢„åŠ è½½ ==================
async def preload_all_api(dataset):
    results = {
        "funding": {}, "p24": {}, "oi": {}, "sentiment": {},
        "oi_hist": {}, "big_pos": {}, "big_acc": {}, "global_acc": {},
    }

    def safe_call(func, *args, **kwargs):
        try:
            return func(*args, **kwargs)
        except:
            return None

    loop = asyncio.get_running_loop()
    executor = _preload_executor
    tasks = []

    for symbol, cycles in dataset.items():
        tasks.append(loop.run_in_executor(executor, safe_call, get_funding_rate, symbol))
        tasks.append(loop.run_in_executor(executor, safe_call, get_24hr_change, symbol))
        tasks.append(loop.run_in_executor(executor, safe_call, get_open_interest, symbol))

    completed = await asyncio.gather(*tasks)
    idx = 0
    for symbol, cycles in dataset.items():
        results["funding"][symbol] = completed[idx]; idx += 1
        results["p24"][symbol] = completed[idx]; idx += 1
        results["oi"][symbol] = completed[idx]; idx += 1

    return results

async def preload_all_api_global(dataset_all):
    unified_dataset = {}
    for batch in dataset_all:
        for symbol, cycles in batch.items():
            if symbol not in unified_dataset:
                unified_dataset[symbol] = {}
            for interval, data in cycles.items():
                if interval not in unified_dataset[symbol]:
                    unified_dataset[symbol][interval] = data
    print(f"ğŸ”„ å…¨å±€é¢„åŠ è½½åˆå¹¶äº† {len(unified_dataset)} ä¸ªå¸ç§")
    return await preload_all_api(unified_dataset)

# ================== JSON æå–ï¼ˆç»Ÿä¸€ç‰ˆï¼‰ ==================
def _extract_decision_block(content: str):
    """æå– <decision> æ ‡ç­¾å†…çš„ JSON åˆ—è¡¨ï¼Œæ”¯æŒ Claude HTML è½¬ä¹‰å½¢å¼"""
    if not content:
        return None

    # 1ï¸âƒ£ å…ˆæŠŠ HTML/Unicode è½¬ä¹‰æ›¿æ¢å›åŸå§‹ç¬¦å·
    content = html.unescape(content)  # \u003c -> <, \u003e -> >

    match = re.search(r"<decision>([\s\S]*?)</decision>", content, flags=re.I)
    if not match:
        return None

    block = match.group(1).strip()
    try:
        parsed = json.loads(block)
        if isinstance(parsed, list):
            return [x for x in parsed if isinstance(x, dict) and "action" in x]
        if isinstance(parsed, dict) and "action" in parsed:
            return [parsed]
    except Exception as e:
        logging.warning(f"âš ï¸ JSON è§£æå¤±è´¥: {e}")
        return None

# åŒç†ä¹Ÿå¯ä»¥æ”¹ _extract_reasoning_blockï¼Œè§£ç  HTML è½¬ä¹‰
def _extract_reasoning_block(content: str):
    """æå– <reasoning> æ ‡ç­¾å†…å®¹ï¼Œæ”¯æŒ HTML è½¬ä¹‰"""
    if not content:
        return None
    content = html.unescape(content)
    match = re.search(r"<reasoning>([\s\S]*?)</reasoning>", content, flags=re.I)
    if not match:
        return None
    return match.group(1).strip()

def _extract_all_json(content: str):
    """
    å°è¯•æå–æ‰€æœ‰å¯èƒ½çš„äº¤æ˜“ä¿¡å· JSONï¼Œ
    å…¼å®¹ DeepSeek / Gemini / Claudeï¼Œæ”¯æŒ HTML/Unicode è½¬ä¹‰
    """
    if not content:
        return None

    # 1ï¸âƒ£ å…ˆå°† HTML / Unicode è½¬ä¹‰è§£ç 
    content = html.unescape(content)  # \u003c -> <, \u003e -> >

    results = []

    # 2ï¸âƒ£ å°è¯•ç›´æ¥è§£ææ•´ä¸ªå†…å®¹
    try:
        parsed = json.loads(content)
        if isinstance(parsed, list):
            return [x for x in parsed if isinstance(x, dict) and "action" in x]
        if isinstance(parsed, dict) and "action" in parsed:
            return [parsed]
    except:
        pass

    # 3ï¸âƒ£ å°è¯•ä» <decision> æ ‡ç­¾ä¸­è§£æ
    decision_match = re.search(r"<decision>([\s\S]*?)</decision>", content, flags=re.I)
    if decision_match:
        block = decision_match.group(1).strip()
        try:
            parsed = json.loads(block)
            if isinstance(parsed, list):
                return [x for x in parsed if isinstance(x, dict) and "action" in x]
            if isinstance(parsed, dict) and "action" in parsed:
                return [parsed]
        except:
            pass

    # 4ï¸âƒ£ åŒ¹é…å•å±‚ JSON å¯¹è±¡çš„è€é€»è¾‘ï¼Œä¿åº•è§£æ
    matches = re.findall(r'\{[^{}]*\}', content, flags=re.S)
    for m in matches:
        try:
            obj = json.loads(m)
            if isinstance(obj, dict) and "action" in obj:
                results.append(obj)
        except:
            pass

    return results if results else None

def merge_market_snapshots(batch_results: list):
    """
    æŠŠå¤šä¸ª batch çš„ formatted_request ä¸­çš„ <JSON> åˆå¹¶æˆä¸€ä¸ª
    é£æ ¼ä¸å• batch å®Œå…¨ä¸€è‡´
    """
    merged = None

    for r in batch_results:
        if not isinstance(r, dict):
            continue

        req = r.get("formatted_request")
        if not req:
            continue

        m = re.search(r"<JSON>([\s\S]*?)</JSON>", req)
        if not m:
            continue

        snapshot = json.loads(html.unescape(m.group(1)))

        if merged is None:
            # ç¬¬ä¸€ä»½ä½œä¸ºéª¨æ¶
            merged = snapshot
        else:
            # åªåˆå¹¶ markets
            merged["markets"].update(snapshot.get("markets", {}))

    return merged

def merge_llm_responses(batch_results: list):
    """
    åˆå¹¶å¤šä¸ª batch çš„ LLM è¿”å›ï¼Œé£æ ¼ä¸å• batch å®Œå…¨ä¸€è‡´
    """
    merged_content = []
    merged_reasoning = []
    merged_signals = []

    http_status = 200
    finish_reason = None

    for r in batch_results:
        if not isinstance(r, dict):
            continue

        if r.get("content"):
            merged_content.append(r["content"])

        if r.get("reasoning"):
            merged_reasoning.append(r["reasoning"])

        if r.get("signals"):
            sigs = r.get("signals") or []
            # è¿‡æ»¤æ‰ Noneã€é dictã€ç¼ºå°‘ symbol æˆ– action çš„ä¿¡å·
            valid_sigs = [
                s for s in sigs 
                if s and isinstance(s, dict) 
                and s.get("symbol") 
                and s.get("action")
            ]
            merged_signals.extend(valid_sigs)

        if r.get("http_status") != 200:
            http_status = r.get("http_status")

        if not finish_reason:
            finish_reason = r.get("finish_reason")

    return {
        "content": "\n\n".join(merged_content) if merged_content else None,
        "reasoning": "\n\n".join(merged_reasoning) if merged_reasoning else None,
        "signals": merged_signals,
        "http_status": http_status,
        "finish_reason": finish_reason,
        "timestamp": time.time()
    }

# ================== æŒä»“æ‹†åˆ† ==================
def split_positions_batch(account, dataset_all, max_symbols=5):
    """
    æ‹†åˆ†æŒä»“æ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡åªåŒ…å«ä¸€éƒ¨åˆ†æŒä»“å¸ç§ + positions + balance_info
    æ”¯æŒéƒ¨åˆ†å¸ç§ç¼ºå¤±æ•°æ®
    """
    positions = account.get("positions", [])
    if not positions:
        print("âš ï¸ å½“å‰è´¦æˆ·æ— æŒä»“")
        return []

    balance_info = {
        "balance": account.get("balance"),
        "available": account.get("available"),
        "total_unrealized": account.get("total_unrealized")
    }

    # æŒä»“å¸ç§å¯¹åº”æ•°æ®
    symbol_data = {}
    for p in positions:
        symbol = p["symbol"]
        if symbol in dataset_all and dataset_all[symbol]:  # åªåŠ å…¥æœ‰æ•°æ®çš„å¸ç§
            symbol_data[symbol] = dataset_all[symbol]
        else:
            print(f"âš ï¸ æŒä»“å¸ç§ {symbol} ç¼ºå°‘æ•°æ®ï¼Œå°†è·³è¿‡")

    symbols = list(symbol_data.keys())
    if not symbols:
        print("âš ï¸ æ‰€æœ‰æŒä»“å¸ç§æ•°æ®ç¼ºå¤±ï¼Œè·³è¿‡æŒä»“æ‹†åˆ†")
        return []

    batches = []
    for i in range(0, len(symbols), max_symbols):
        batch_symbols = symbols[i:i+max_symbols]
        batch = {"positions": positions, "balance_info": balance_info}
        for s in batch_symbols:
            batch[s] = symbol_data[s]
        batches.append(batch)

    print(f"âœ… æ‹†åˆ†æŒä»“æ‰¹æ¬¡æ•°é‡: {len(batches)}")
    return batches

# ================== æ‰¹æ¬¡æ‹†åˆ† ==================
def split_dataset_by_symbol_limit(dataset: dict, max_symbols=5):
    """
    æ‹†åˆ†éæŒä»“å¸ç§æ‰¹æ¬¡ï¼Œæ¯æ‰¹æœ€å¤š max_symbols ä¸ªå¸ç§
    æ”¯æŒéƒ¨åˆ†å¸ç§ç¼ºå°‘æ•°æ®
    """
    batches = []

    symbols = [k for k in dataset.keys() if k not in ("positions", "balance_info")]
    items = [(k, dataset[k]) for k in symbols if dataset[k]]  # åªä¿ç•™æœ‰æ•°æ®çš„å¸ç§

    if not items:
        print("âš ï¸ éæŒä»“å¸ç§æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ‹†åˆ†")
        return batches

    for i in range(0, len(items), max_symbols):
        batch = dict(items[i:i + max_symbols])
        batches.append(batch)

    print(f"âœ… æ‹†åˆ†éæŒä»“æ‰¹æ¬¡æ•°é‡: {len(batches)}")
    return batches

# ================== æ•°æ®æ ¼å¼åŒ– ==================
def _build_dataset_json(dataset, preloaded=None):
    """æ„å»ºç»“æ„åŒ– JSON æ•°æ® - å¯¹æ ‡æŠ•å–‚æ•°æ®ç»“æ„"""
    account = account_snapshot
    positions = dataset.get("positions", [])
    balance_info = dataset.get("balance_info") or {
        "balance": account.get("balance"),
        "available": account.get("available"),
        "total_unrealized": account.get("total_unrealized")
    }
    symbols_in_batch = [k for k in dataset.keys() if k not in ("positions", "balance_info")]
    
    # è¿‡æ»¤æ‰¹æ¬¡å†…æŒä»“
    if positions and symbols_in_batch:
        positions = [p for p in positions if p["symbol"] in symbols_in_batch]
    
    # æ„å»º global_context
    try:
        from global_context import build_global_context, get_open_limit_orders
        global_context = build_global_context(symbols_in_batch)
        open_limit_orders = get_open_limit_orders()
    except Exception:
        global_context = {}
        open_limit_orders = []
    
    output = {
        "timestamp": time.strftime('%Y-%m-%d %H:%M:%S UTC', time.gmtime()),
        "data_freshness": "realtime",  # æ ‡è®°æ•°æ®åŒ…å«å®æ—¶ä¿¡æ¯
        "balance_info": {
            "balance": float(balance_info.get("balance", 0)),
            "available": float(balance_info.get("available", 0)),
            "total_unrealized": float(balance_info.get("total_unrealized", 0))
        },
        "global_context": global_context,
        "open_limit_orders": open_limit_orders,
        "positions": [],
        "markets": {}
    }
    
    # æ„å»ºæŒä»“æ•°æ®ï¼ˆåŒ…å«å³°å€¼ç›ˆåˆ©å’Œå›æ’¤ä¿¡æ¯ï¼‰
    from trade_tracker import get_active_trade
    for p in positions:
        symbol = p["symbol"]
        size = float(p["size"])
        side = "LONG" if size > 0 else "SHORT"
        entry_price = float(p["entry"])
        mark_price = float(p["mark_price"])
        
        # è®¡ç®—å½“å‰ç›ˆäºç™¾åˆ†æ¯”
        if side == "LONG":
            pnl_pct = round((mark_price - entry_price) / entry_price * 100, 2)
        else:
            pnl_pct = round((entry_price - mark_price) / entry_price * 100, 2)
        
        # è·å–æ´»è·ƒäº¤æ˜“è®°å½•ï¼ˆåŒ…å«å³°å€¼å’Œå›æ’¤ï¼‰
        trade_stats = get_active_trade(symbol, side)
        peak_pnl_pct = 0
        drawdown_from_peak_pct = 0
        hold_minutes = 0
        
        if trade_stats:
            peak_price = trade_stats.get("peak_price", entry_price)
            entry_time = trade_stats.get("entry_time", time.time())
            hold_minutes = int((time.time() - entry_time) / 60)
            
            # è®¡ç®—å³°å€¼ç›ˆäºç™¾åˆ†æ¯”
            if side == "LONG":
                peak_pnl_pct = round((peak_price - entry_price) / entry_price * 100, 2)
            else:
                peak_pnl_pct = round((entry_price - peak_price) / entry_price * 100, 2)
            
            # è®¡ç®—ä»å³°å€¼çš„å›æ’¤ç™¾åˆ†æ¯”
            if peak_pnl_pct > 0:
                drawdown_from_peak_pct = round((peak_pnl_pct - pnl_pct) / peak_pnl_pct * 100, 1)
        
        pos_data = {
            "symbol": symbol,
            "side": side,
            "size": abs(size),
            "entry": entry_price,
            "mark_price": mark_price,
            "pnl": float(p["pnl"]),
            "position_value": abs(size) * mark_price,
            "pnl_pct": pnl_pct,
            # æ–°å¢ï¼šå³°å€¼å’Œå›æ’¤ä¿¡æ¯ï¼ˆå¸®åŠ© AI åˆ¤æ–­æ˜¯å¦åº”è¯¥æ­¢ç›ˆï¼‰
            "peak_pnl_pct": peak_pnl_pct,
            "drawdown_from_peak_pct": drawdown_from_peak_pct,
            "hold_minutes": hold_minutes,
            "tp_sl": [f"{o['type']}={o['stopPrice']}" 
                     for o in tp_sl_cache.get(symbol, {}).get(side, [])],
            "last_update_time": time.time()
        }
        output["positions"].append(pos_data)
    
    # æ„å»ºå¸‚åœºæ•°æ®
    for symbol in symbols_in_batch:
        cycles = dataset[symbol]
        fr = preloaded.get("funding", {}).get(symbol)
        p24 = preloaded.get("p24", {}).get(symbol)
        oi_now = preloaded.get("oi", {}).get(symbol)
        
        # è·å–å®æ—¶ä»·æ ¼
        realtime_price = None
        try:
            rp = redis_client.get(f"realtime_price:{symbol}")
            if rp:
                realtime_price = float(rp)
        except:
            pass
        
        # ä½¿ç”¨å®æ—¶ä»·æ ¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨24hæ•°æ®
        current_price = realtime_price or (p24['lastPrice'] if p24 else None)
        
        market_data = {
            "price": current_price,
            "realtime_price": realtime_price,  # æ˜ç¡®æ ‡è®°å®æ—¶ä»·æ ¼
            "24h_high": p24['highPrice'] if p24 else None,
            "24h_low": p24['lowPrice'] if p24 else None,
            "24h_change_pct": p24['priceChangePercent'] if p24 else None,
            "24h_volume_usd": round(p24['quoteVolume'] / 1e6, 2) if p24 else None,
            "funding_rate": fr,
            "open_interest": oi_now,
            "timeframes": {}
        }
        
        for interval in cycles.keys():
            data = cycles[interval]
            ind = data.get("indicators") or {}
            
            # æ„å»ºæŒ‡æ ‡æ•°æ®ï¼Œä¿ç•™å®Œæ•´ç»“æ„
            indicators_out = {}
            for k, v in ind.items():
                if isinstance(v, float):
                    indicators_out[k] = round(v, 6)
                elif isinstance(v, dict):
                    indicators_out[k] = v
                elif isinstance(v, list):
                    indicators_out[k] = v
                else:
                    indicators_out[k] = v
            
            # è·å–å½“å‰æœªæ”¶ç›˜Kçº¿
            current_candle = None
            try:
                cc_raw = redis_client.get(f"current_candle:{symbol}:{interval}")
                if cc_raw:
                    current_candle = json.loads(cc_raw)
            except:
                pass
            
            tf_data = {
                "indicators": indicators_out
            }
            
            # æ·»åŠ å½“å‰Kçº¿ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if current_candle:
                tf_data["current_candle"] = {
                    "open": current_candle.get("Open"),
                    "high": current_candle.get("High"),
                    "low": current_candle.get("Low"),
                    "close": current_candle.get("Close"),
                    "volume": current_candle.get("Volume"),
                    "is_closed": False,
                    "seconds_to_close": current_candle.get("seconds_to_close", 0)
                }
            
            market_data["timeframes"][interval] = tf_data

        output["markets"][symbol] = market_data
    
    return output

def build_llm_user_prompt(market_snapshot: dict) -> str:
    """
    æŠŠâ€œçº¦æŸæ–‡å­— + JSONâ€ç»„è£…æˆä¸€æ¬¡ LLM çš„ user content
    """
    return f"""
Below is the current account status and market snapshot data (JSON).

This is a formal trading decision request.
Please strictly follow the [System Instructions] to complete the multi-timeframe analysis and output a single, unique trading action.

[Current Account Constraints]
- Available account balance is limited; overtrading is prohibited; capital must grow steadily
- Excessive trading and frequent repetitive entries are strictly prohibited
- If risk or structure is unclear, choosing wait or hold is permitted

[Current Account and Market Data]
<JSON>
{json_safe_dumps(market_snapshot)}
</JSON>
""".strip()

# ================== AIBTC.VIP æ‰¹é‡æŠ•å–‚ ==================
async def _push_single_batch_claude(dataset, preloaded, batch_idx, total_batches):
    """
    ä½¿ç”¨ AIBTC.VIP æ¨¡å‹è¿›è¡Œæ‰¹æ¬¡æŠ•å–‚ï¼Œå…¼å®¹ Unicode/HTML è½¬ä¹‰ï¼Œä¿è¯ signals å®Œæ•´
    """
    loop = asyncio.get_running_loop()
    json_data = await loop.run_in_executor(None, _build_dataset_json, dataset, preloaded)
    user_prompt = await loop.run_in_executor(None, build_llm_user_prompt, json_data)
    system_prompt = await loop.run_in_executor(None, _read_prompt)

    payload = {
        "model": CLAUDE_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0,
        "max_tokens": 8000,
        "stream": False
    }

    max_retries = 3
    base_timeout = 15  # è¶…æ—¶åŸºå‡†ç§’æ•°

    for attempt in range(max_retries):
        attempt_start = time.perf_counter()
        current_timeout = base_timeout * (attempt + 1)

        try:
            session = await get_http_session(CLAUDE_URL)

            async with session.post(
                CLAUDE_URL,
                json=payload,
                headers={"Authorization": f"Bearer {CLAUDE_API_KEY}"},
                timeout=aiohttp.ClientTimeout(total=current_timeout)
            ) as resp:

                status = resp.status

                if status != 200:
                    raise aiohttp.ClientError(f"HTTP {status}")

                raw_text = await resp.text()
                attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)

                print(
                    f"âœ… AIBTC.VIP æ‰¹æ¬¡ {batch_idx} ç¬¬{attempt+1}æ¬¡è¿”å› | "
                    f"{attempt_time}ms | HTTP {status}"
                )

                content = None
                reasoning = None
                signals = []
                raw_json = None
                finish_reason = None

                try:
                    raw_json = json.loads(raw_text)
                    choice = raw_json.get("choices", [{}])[0]
                    content = choice.get("message", {}).get("content")
                    finish_reason = choice.get("finish_reason")
                    reasoning = _extract_reasoning_block(content)

                    if content:
                        from html import unescape
                        content_decoded = unescape(content)
                        signals = _extract_all_json(content_decoded) or []

                except Exception as parse_err:
                    logging.warning(f"âš ï¸ AIBTC.VIP JSON è§£æå¤±è´¥: {parse_err}")

                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,  # â­ å·²å®‰å…¨åºåˆ—åŒ–
                    "content": content,
                    "reasoning": reasoning,
                    "signals": signals,
                    "raw_text": raw_text,
                    "raw_json": raw_json,
                    "finish_reason": finish_reason,
                    "http_status": status,
                    "ts": time.time(),
                    "attempt": attempt + 1,
                    "response_time_ms": attempt_time
                }

        except asyncio.TimeoutError:
            attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)
            print(f"â±ï¸ AIBTC.VIP æ‰¹æ¬¡ {batch_idx} ç¬¬{attempt+1}æ¬¡è¶…æ—¶ ({attempt_time}ms)")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 * (attempt + 1))
                continue
            else:
                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,
                    "signals": [],
                    "raw_text": None,
                    "raw_json": None,
                    "finish_reason": None,
                    "http_status": None,
                    "error": f"æ‰¹æ¬¡ {batch_idx} åœ¨{max_retries}æ¬¡å°è¯•åè¶…æ—¶",
                    "ts": time.time(),
                    "attempt": max_retries,
                    "response_time_ms": attempt_time
                }

        except aiohttp.ClientError as e:
            attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)
            print(f"ğŸŒ Claude æ‰¹æ¬¡ {batch_idx} ç¬¬{attempt+1}æ¬¡ç½‘ç»œé”™è¯¯: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(3 * (attempt + 1))
                continue
            else:
                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,
                    "signals": [],
                    "raw_text": None,
                    "raw_json": None,
                    "finish_reason": None,
                    "http_status": None,
                    "error": f"æ‰¹æ¬¡ {batch_idx} ç½‘ç»œé”™è¯¯: {e}",
                    "ts": time.time(),
                    "attempt": max_retries,
                    "response_time_ms": attempt_time
                }

        except Exception as e:
            attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)
            error_msg = f"æ‰¹æ¬¡ {batch_idx} æœªçŸ¥é”™è¯¯: {e}"
            print(f"âŒ {error_msg}")

            if attempt < max_retries - 1:
                await asyncio.sleep(2 * (attempt + 1))
                continue
            else:
                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,
                    "signals": [],
                    "raw_text": None,
                    "raw_json": None,
                    "finish_reason": None,
                    "http_status": None,
                    "error": error_msg,
                    "ts": time.time(),
                    "attempt": max_retries,
                    "response_time_ms": attempt_time
                }

# ================== å°ç±³ MiMo æ‰¹é‡æŠ•å–‚ ==================
async def _push_single_batch_mimo(dataset, preloaded, batch_idx, total_batches):
    """
    ä½¿ç”¨å°ç±³ MiMo æ¨¡å‹è¿›è¡Œæ‰¹æ¬¡æŠ•å–‚
    """
    loop = asyncio.get_running_loop()
    json_data = await loop.run_in_executor(None, _build_dataset_json, dataset, preloaded)
    user_prompt = await loop.run_in_executor(None, build_llm_user_prompt, json_data)
    system_prompt = await loop.run_in_executor(None, _read_prompt)

    payload = {
        "model": MIMO_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.1,  # é™ä½æ¸©åº¦ï¼Œå†³ç­–æ›´ç¨³å®š
        "max_completion_tokens": 8000,
        "stream": False,
        "top_p": 0.9,  # ç•¥å¾®é™ä½
        "frequency_penalty": 0,
        "presence_penalty": 0,
        "thinking": {"type": "disabled"}
    }

    max_retries = 3
    base_timeout = 15

    for attempt in range(max_retries):
        attempt_start = time.perf_counter()
        current_timeout = base_timeout * (attempt + 1)

        try:
            session = await get_http_session(MIMO_URL)

            async with session.post(
                MIMO_URL,
                json=payload,
                headers={"Authorization": f"Bearer {MIMO_API_KEY}"},
                timeout=aiohttp.ClientTimeout(total=current_timeout)
            ) as resp:

                status = resp.status

                if status != 200:
                    raise aiohttp.ClientError(f"HTTP {status}")

                raw_text = await resp.text()
                attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)

                print(
                    f"âœ… MiMo æ‰¹æ¬¡ {batch_idx} ç¬¬{attempt+1}æ¬¡è¿”å› | "
                    f"{attempt_time}ms | HTTP {status}"
                )

                content = None
                reasoning = None
                signals = []
                raw_json = None
                finish_reason = None

                try:
                    raw_json = json.loads(raw_text)
                    choice = raw_json.get("choices", [{}])[0]
                    content = choice.get("message", {}).get("content")
                    finish_reason = choice.get("finish_reason")
                    reasoning = _extract_reasoning_block(content)

                    if content:
                        from html import unescape
                        content_decoded = unescape(content)
                        signals = _extract_all_json(content_decoded) or []

                except Exception as parse_err:
                    logging.warning(f"âš ï¸ MiMo JSON è§£æå¤±è´¥: {parse_err}")

                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,
                    "content": content,
                    "reasoning": reasoning,
                    "signals": signals,
                    "raw_text": raw_text,
                    "raw_json": raw_json,
                    "finish_reason": finish_reason,
                    "http_status": status,
                    "ts": time.time(),
                    "attempt": attempt + 1,
                    "response_time_ms": attempt_time
                }

        except asyncio.TimeoutError:
            attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)
            print(f"â±ï¸ MiMo æ‰¹æ¬¡ {batch_idx} ç¬¬{attempt+1}æ¬¡è¶…æ—¶ ({attempt_time}ms)")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 * (attempt + 1))
                continue
            else:
                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,
                    "signals": [],
                    "raw_text": None,
                    "raw_json": None,
                    "finish_reason": None,
                    "http_status": None,
                    "error": f"æ‰¹æ¬¡ {batch_idx} åœ¨{max_retries}æ¬¡å°è¯•åè¶…æ—¶",
                    "ts": time.time(),
                    "attempt": max_retries,
                    "response_time_ms": attempt_time
                }

        except aiohttp.ClientError as e:
            attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)
            print(f"ğŸŒ MiMo æ‰¹æ¬¡ {batch_idx} ç¬¬{attempt+1}æ¬¡ç½‘ç»œé”™è¯¯: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(3 * (attempt + 1))
                continue
            else:
                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,
                    "signals": [],
                    "raw_text": None,
                    "raw_json": None,
                    "finish_reason": None,
                    "http_status": None,
                    "error": f"æ‰¹æ¬¡ {batch_idx} ç½‘ç»œé”™è¯¯: {e}",
                    "ts": time.time(),
                    "attempt": max_retries,
                    "response_time_ms": attempt_time
                }

        except Exception as e:
            attempt_time = round((time.perf_counter() - attempt_start) * 1000, 2)
            error_msg = f"æ‰¹æ¬¡ {batch_idx} æœªçŸ¥é”™è¯¯: {e}"
            print(f"âŒ {error_msg}")

            if attempt < max_retries - 1:
                await asyncio.sleep(2 * (attempt + 1))
                continue
            else:
                return {
                    "batch_idx": batch_idx,
                    "formatted_request": user_prompt,
                    "signals": [],
                    "raw_text": None,
                    "raw_json": None,
                    "finish_reason": None,
                    "http_status": None,
                    "error": error_msg,
                    "ts": time.time(),
                    "attempt": max_retries,
                    "response_time_ms": attempt_time
                }

# ================== é€šç”¨æ‰¹é‡æŠ•å–‚ ==================
async def push_batch_to_ai():
    if not _is_ready_for_push():
        return None

    start_total = time.perf_counter()

    dataset_all = batch_cache.copy()
    batch_cache.clear()
    all_signals = []

    account = account_snapshot

    # --- 1. æ‹†åˆ†æŒä»“æ‰¹æ¬¡ ---
    positions_batches = split_positions_batch(account, dataset_all, max_symbols=5)
    positions_symbols = [
        p["symbol"] for batch in positions_batches for p in batch.get("positions", [])
    ]

    # --- 2. æ‹†åˆ†éæŒä»“å¸ç§ ---
    symbol_dataset = {k: v for k, v in dataset_all.items() if k not in positions_symbols}
    symbol_batches = split_dataset_by_symbol_limit(symbol_dataset, max_symbols=5)

    # --- 3. åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ ---
    batches = positions_batches + symbol_batches

    # --- 4. é¢„åŠ è½½ ---
    preloaded_batches = []
    for batch in batches:
        symbols_only = {k: v for k, v in batch.items() if k not in ("positions", "balance_info")}
        preloaded = await preload_all_api(symbols_only) if symbols_only else {}
        preloaded_batches.append(preloaded)

    # --- 5. åˆ›å»ºæŠ•å–‚ä»»åŠ¡ ---
    tasks = []
    for idx, batch in enumerate(batches):
        preloaded = preloaded_batches[idx]
        if AI_PROVIDER == "claude":
            tasks.append(
                _push_single_batch_claude(batch, preloaded, idx + 1, len(batches))
            )
        elif AI_PROVIDER == "mimo":
            tasks.append(
                _push_single_batch_mimo(batch, preloaded, idx + 1, len(batches))
            )
        else:
            raise ValueError(f"æœªçŸ¥ AI_PROVIDER: {AI_PROVIDER}")

    # --- 6. æ‰§è¡ŒæŠ•å–‚ ---
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # --- ç»Ÿè®¡ ---
    success_count = 0
    timeout_count = 0
    total_elapsed_time = 0
    success_response_time = 0

    for r in results:
        if not isinstance(r, dict):
            timeout_count += 1
            continue

        rt = r.get("response_time_ms", 0)
        total_elapsed_time += rt

        if r.get("http_status") == 200:
            success_count += 1
            success_response_time += rt
        elif "è¶…æ—¶" in (r.get("error") or ""):
            timeout_count += 1

    valid_results = [r for r in results if isinstance(r, dict)]
    valid_count = len(valid_results)
    overall_avg = total_elapsed_time / valid_count if valid_count else 0

    print(
        f"ğŸ“Š è¯·æ±‚ç»Ÿè®¡: æˆåŠŸ {success_count}/{valid_count} | "
        f"è¶…æ—¶ {timeout_count} | "
        f"æ•´ä½“å¹³å‡è€—æ—¶ {overall_avg:.0f}ms | "
        f"æˆåŠŸå¹³å‡è€—æ—¶ {success_response_time / success_count if success_count else 0:.0f}ms"
    )

    # ================== âœ… Redisï¼šå•æ¬¡æŠ•å–‚é£æ ¼åˆå¹¶ ==================

    round_ts = time.time()

    # -------- KEY_REQï¼šåˆå¹¶åçš„å®Œæ•´ prompt --------
    merged_snapshot = merge_market_snapshots(results)

    if merged_snapshot:
        merged_user_prompt = build_llm_user_prompt(merged_snapshot)

        redis_client.rpush(
            KEY_REQ,
            json_safe_dumps({
                "timestamp": round_ts,
                "request": merged_user_prompt
            })
        )

    # -------- KEY_RESï¼šåˆå¹¶åçš„å®Œæ•´æ¨¡å‹å›å¤ --------
    merged_response = merge_llm_responses(results)

    redis_client.rpush(
        KEY_RES,
        json_safe_dumps(merged_response)
    )

    # æ±‡æ€» signalsï¼ˆç»™å‡½æ•°è¿”å›å€¼ç”¨ï¼‰
    for r in results:
        if isinstance(r, dict):
            sigs = r.get("signals") or []
            # è¿‡æ»¤æ‰ Noneã€é dictã€ç¼ºå°‘ symbol æˆ– action çš„ä¿¡å·
            valid_sigs = [
                s for s in sigs 
                if s and isinstance(s, dict) 
                and s.get("symbol") 
                and s.get("action")
            ]
            all_signals.extend(valid_sigs)

    end_total = time.perf_counter()
    print(
        f"ğŸ“Š è¯·æ±‚ç»Ÿè®¡: æŠ•å–‚æ‰¹æ¬¡ {len(results)} | "
        f"æ€»è€—æ—¶ {round((end_total - start_total), 2)} ç§’"
    )

    return all_signals if all_signals else None

# åˆ«åä¿ç•™
push_batch_to_deepseek = push_batch_to_ai